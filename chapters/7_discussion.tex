% Grammar check needed.
\section{Cost Model}\label{sec:costModel}
In the previous section, we gave a subjective description of use cases where
using our solution may be more effective. In this section, we provide a
more formal comparison between architectures that separates
compute and storage, and the architectures that do not. Let us assume that we
have a graph database which is defined by two variables: its storage requirements and
its computational requirements. The storage requirement can be specified in
terms of the number of bytes that the underlying data needs. However,
computational requirements depend on various factors like the type of query,
and database architecture. Therefore, for simplification, we will consider
`graph access per second' as a proxy for the computational requirements. Graph
access per second means the number of times we look up a vertex's
neighbours per second. Therefore, we will use $A$ as the graph accesses that
need to be performed every second, and $S$ as the storage that the underlying
graph requires.

\smallskip
Now, we will consider the cost of running this database with AWS S3 as the
storage layer. Let the cost of storing data on AWS S3 be $p_{s3}$ \$/GB/second. Now,
we need to take into account the cost of running an EC2 instance for performing
the required computations. Let us assume that a single EC2 instance can service
$a_{s3}$ graph accesses per second, and that this instance costs $p_{i}$
\$/second to run. Finally, there is a cost for sending a request to AWS S3 which
we will assume to be $p_r$ \$/request. With these variables, the cost of
running the workload for one second would be:

\begin{equation}
    \label{eqn:s3Abstract}
    \bigl(S.p_{s3}\bigr) + \bigl(\Bigg\lceil \frac{A}{a_{s3}} \Bigg\rceil . p_i + p_r.A\bigr)
\end{equation}

In equation \ref{eqn:s3Abstract}, the first component represents the storage
cost and the rest of the equation represents the computational component. The
first component $S.p_{s3}$ is the cost of storing the underlying data. In the
second component, we find the number of instances that we need to spawn based on
the required throughput of graph accesses, and then we multiply that with the
cost of a single instance. Finally, we have the cost of sending requests to S3
which is $p_r.A$. Although it is possible to group multiple requests to a single
object in a request, we consider that each graph access is done with a separate
request.

\smallskip
Now, we will formulate the cost for running the workload using a system that 
keeps the entirety of its data in an SSD. Let us assume that this instance is
able to sustain a load of $a_{SSD}$ graph accesses per second and the cost of
this instance is $p_j$ \$/second. For this system, these are the only two
metrics we need to consider the cost of running the system for one second, which
is:
\begin{equation}
    \label{eqn:ssdAbstract}
    \Bigg\lceil \frac{A}{a_{SSD}} \Bigg\rceil . p_j
\end{equation}

Note that equation \ref{eqn:ssdAbstract} does not consider the storage cost $S$
in the equation. This is because the storage cost is subsumed within $p_j$
because we need an instance which has enough capacity to hold the entire
database. Therefore, the $p_j$ depends on the storage as follows:
\begin{equation*}
    p_j &= \min_{i_{\text{cost}}}\big\{ i \in I: i_\text{storage} \ge S\big\}
\end{equation*}
In this equation, $I$ is the set of all available instances and we get the price
of the cheapest possible instance which has enough storage to store the entire
graph.

\medskip
We see that equation \ref{eqn:s3Abstract} has two separate components which
scale independently when the storage or the throughput requirements change,
whereas equation \ref{eqn:ssdAbstract} only has a single component which
subsumes both the throughput and the storage requirements. By looking at these
equations, it would appear that using AWS S3 is more costly because we
essentially have a component related to the instance cost in both equations but
AWS S3 has additional costs for storage and making request. This would indeed be
the case if cloud providers provided all possible permutations of compute and
storage so that we would not waste any money while picking $p_j$. However, this
is not the case; usually instances with more compute also have bigger SSDs. This
may force us to pay for more compute than we actually need. We will now look at
some concrete use cases and elucidate the difference between the cost for both
approaches.

\subsection{Use cases}
We will now use equations \ref{eqn:s3Abstract} and \ref{eqn:ssdAbstract} to talk
about use cases where separating storage and compute makes more sense than the
model of coupled storage and compute. Before we introduce the use cases, we have
to make some educated guesses about the accesses per second that AWS S3 and SSDs
can serve. To do this, we ran our solution with both backends: AWS S3 and SSDs.
We found that a c7gn.xlarge instance can make and process around 5
thousand accesses per second while a instance with SSD can make and process
around 50 thousand accesses per second. The exact value of these numbers may
vary from one setup to another but it is generally accepted that a single
instance would be able to make fewer network calls compared to SSD accesses.
With these numbers, we will now consider two use cases to highlight where AWS S3
should be chosen over tools that have coupled storage and compute.

\medskip
\textbf{Knowledge Graphs} are becoming an increasingly popular use cases for
graph databases. Knowledge graph represent various knowledge concepts using a
graph structure where related concepts have an edge between them. In such
graphs, traversals may be used to find related concepts or to explore a common
link between two concepts. Let us assume we have a knowledge graph that is used
by a pharmaceutical company to model relationships between studies, populations,
diseases, etc. We assume that the size of this graph is approximately 5 TB and
that the usage of this database by scientists can be satisfied by 1 thousand
graph accesses per second. Table \ref{table:costKnowledgeGraphs} shows the total
cost for both storage mediums for this case. The cheapest general purpose
instance which has an instance store of 5 TB is `m6id.24xlarge' which costs 5.7
dollars per hour. We notice that, with AWS S3, we were able to use a much
cheaper instance because we do not require a very high throughput. Whereas, if
we use SSD, we are forced to use an instance which has 96 vCPUs although we may
not need that much compute.

\begin{table}[h!]
 \centering
 \begin{tabular}{|c | c | c |} 
 \hline
  & AWS S3 & SSD \\ [0.5ex] 
 \hline\hline
     \textbf{Compute} & 0.25 & 5.7 \\ 
     \textbf{Storage} & 0.17 & Na \\
     \textbf{Access} &  1.44 & Na \\
     \hline
     \textbf{Total} & 1.86 & 5.7 \\
 \hline
 \end{tabular}
 \caption{Cost for first use case (\$ per hour)}
 \label{table:costKnowledgeGraphs}
\end{table}

\medskip
\textbf{Fraud detection} is a common use case for graph databases as it enables
security services to check whether a transaction is related to a bad actor or
not. These systems can have a large database of information as they contain
links between people, devices, and accounts. These systems can also have a very
high workload if they verify every transaction that takes place. Therefore, if
we assume, similar to the previous use case, that we have a graph that is
5 TB in size and requires 100 thousand graph accesses per second to check for
frauds. Table \ref{table:costFraudDetection} shows the total cost for both
storage mediums for this case. We observe that using AWS S3 in this case is
almost 10 times more expensive than using two instances with a total storage of
5 TB. The problem with using AWS S3 in this case is that the cost of making
requests is so high that it offsets any benefit that we get by using
cheaper instances.

\begin{table}[h!]
 \centering
 \begin{tabular}{|c | c | c |} 
 \hline
  & AWS S3 & SSD \\ [0.5ex] 
 \hline\hline
     \textbf{Compute} & 5 & 11.4 \\ 
     \textbf{Storage} & 0.17 & Na \\
     \textbf{Access} &  144 & Na \\
     \hline
     \textbf{Total} & 149.17 & 11.4 \\
 \hline
 \end{tabular}
 \caption{Cost for second use case (\$ per hour)}
 \label{table:costFraudDetection}
\end{table}

\subsection{Other cost considerations}
Apart from the cost of infrastructure, there are other aspects of a system that
need to be taken into account when using a graph database. Although these
consideration are hard to capture within a cost model, we will give a subjective
description of some of these additional costs.
\begin{enumerate}
    \item \textbf{Cost of update mechanisms}: Although we have only considered a
        read-only workload for the cost model, real databases also need to
        provide support for transactions. The mechanism for supporting ACID
        transactions will likely differ between the two approaches. Therefore,
        it may incur different infrastructure and maintainance costs which are
        important to consider.
    \item \textbf{Migration cost}: When using an SSD for storing a graph, the
        graph may grow to an extent that the user may have to upgrade an
        instance to accomodate the graph. This migration of a production
        workload is tricky and brings with it various challenges like ensuring
        data durability, and minimizing the downtime. These efforts require
        additional labor cost which should be taken into account.
    \item \textbf{Replication cost}: In order to ensure availability, production
        workloads are often replicated across various nodes in the coupled
        storage and compute model. This not only requires extra infrastructure
        but also requires data replication between instances. These replication
        mechanisms add additional complexity which is relegated to AWS S3 if we
        use it as the storage layer.
\end{enumerate}

\medskip
In conclusion, this section provides a mathematical model for reasoning about
the cost of using AWS S3 instead of SSD for graph storage. Then, we use this
model to provide use cases where one may be preferred over this other. Overall,
we see that AWS S3 is better for use cases where the amount of data is huge but
the required throughput is not as large. We also mention some additional costs
that may be incurred when using SSD as the storage layer. Therefore, using AWS S3 is
not ideal for all use cases pertaining to querying graphs, but it does have
significant benefits over other approaches for certain use cases we mentioned in
this section.

\section{Threats to credibility}\label{sec:threats}
In this section, we discuss some threats to the credibility of this work. We
first discuss why extending the current architecture to support updates might
affect the appeal of our solution. Then, we summarize the assumptions we have
made about the possible users of this system and why that requires more
validation. 

\subsection{Performing Updates}
For a graph database, it is essential to be able to handle updates and support
transactions. In 2008, Brantner et al.\cite{brantner2008building} proposed an
update mechanism for B-trees stored in AWS S3. This architecture utilized AWS
Simple Queue Service (SQS) to store updates, and these updates were flushed back
to S3 at regular intervals. Their approach can be modified to handle updates to
CSR file partitions. However, this approach incurs an additional cost of using
AWS SQS and it also requires computational resources to flush the updates. It
may also be possible to implement a multi-tenant storage service like
Neon\cite{neonPostgres}. In this architecture, the graph access service may
become part of a multi-tenant storage layer which handles updates.

Both of the approaches mentioned above might affect the performance and the cost
of the system. Therefore, it is possible that the cost of the infrastructure
required to perform updates might become untenable, or that the performance
degradation due to handling updates might be severe. In the future, the
performance and cost of performing updates within the proposed architecture
would need to be explored. However, existence of many popular relational
databases like Neon\cite{neonPostgres} and Snowflake\cite{snowflake} that follow
a similar architecture suggests that it is likely possible to handle updates in
a performant and cost-effective manner.

\subsection{Real world applications}
While discussing the cost model, we mentioned that our system is cost effective
for large graphs which only need a few thousand accesses per second. However, as
mentioned in Section-\ref{sec:costModel}, our solution is not cost effective for
small graphs that are only a few gigabytes in size or require hundreds of
thousands of accesses per second. Recent research has shown that very large
graphs are quite common in both industry and academia\cite{sahu2017ubiquity},
however we do not know the throughput requirements of these use cases. There is
no research indicating that there is a positive correlation between the size of
a database and its throughput requirements. Therefore, the null hypothesis would
be to assume that there would be many use cases where large graph databases have
low throughput requirements.

\smallskip
Our research also assumes that the additional latency is acceptable to
the users of the system. This may be applicable for graph databases that are
queried by human actors. This is because, the difference between single digit
millisecond latency and latency of a few hundred milliseconds is barely
perceptable to users. However, there are many use cases where non-human actors
query a database. Examples might include machine learning or fraud detection
algorithms. In such cases, a latency increase of an order of magnitude may not
be acceptable. However, we note that if the graph is sharded across multiple
machines then the difference between performance might not be substantial as the
cost of network communication would be incurred in either cases. Therefore, it
is likely but not certain that there are use cases where the additional latency
is acceptable.
