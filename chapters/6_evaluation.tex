Before presenting the results of our comparison with the baseline, we will
describe our benchmarking procedure. Within the benchmarking methodology, we
highlight the queries we run along with the dataset on which we run the
queries. Finally, we describe how the services are deployed and how the results
are collected.

\smallskip
Our benchmark contains a total of seven different types of queries. Each of
these seven queries starts with a random source vertex and performs the desired
traversal according to the traversal specification. A traversal specification
specifies the label and direction that needs to be followed for every hop. For
example, a traversal specification might instruct the algorithm to start from a
node of type `Person', traverse the outgoing edges with the label
`PersonKnows', and then find all the incoming edges for `PostCreator'. As a
result of running this traversal, we would get the identifiers for all the posts
created by a person's friends. The schema containing the specification of the
nodes and labels mentioned here can be found in Appendix \ref{sec:schema}. This
schema describes a social network consisting of people who can create posts
within forums, comment on posts, etc. 

\smallskip
The seven traversals that we have chosen can be divided into four parts. The
first two traversals are 1-Hop traversals, which means they only access a node's
immediate neighbours. Similarly, we have two queries for 2-Hop traversals, and
3-Hop traversals. One of the two queries in each category is chosen in such a
manner that the result cardinality is close to one, and the other query has a
higher cardinality. However, for 4-Hop traversals, the baseline implementation
was so slow that we do not have a high cardinality traversal for 4-Hops. The
following list contains the details of the queries that were used for the
evaluation. Each of these queries was repeated 20 times with different source
nodes chosen at random.
\begin{enumerate}
    \item \textbf{1 Hop}
        \begin{enumerate}
            \item \textbf{Q-1}: Start with a node of type `Forum', and traverse outgoing
                edges with the label `ForumMember'. This gives us all the forum members for
                a particular forum.
            \item \textbf{Q-2}: Start with a node of type `Post', and traverse outgoing
                edges with the label `PostContainerForum'. This gives us the container forum
                for a post.
        \end{enumerate}
    \item \textbf{2 Hop}
        \begin{enumerate}
            \item \textbf{Q-3}: Start with a node of type `Person', and traverse
                the edges with the label `PersonKnows' twice. This gives us all the
                people that a person's friends know.
            \item \textbf{Q-4}: Start with a node of type `Person', and traverse
                the edge with the label `PersonWorksAt'. After this traverse the
                relation `OrgLocation'. This gives us the locations of the
                companies that a person has worked for.
        \end{enumerate}
    \item \textbf{3 Hop}
        \begin{enumerate}
            \item \textbf{Q-5}: Start with a node of type `Person', and traverse
                the edges with the label `PersonKnows'. After this, traverse the
                edge with the label `PersonInterestTag' to get the interests of all
                these people. Finally, traverse edges with the label `TagClass' to
                find the tags for each of these interests. This query gives us
                the tags for the interests of a person's friends.
            \item \textbf{Q-6}: Start with a node of type `Post', and traverse
                the edge with the label `PostContainerForum' to get the forum which
                the post is a part of. Then we traverse the `ForumTag' label
                followed by the `TagClass' label. This gives us the tag classes for
                a post's forum.
        \end{enumerate}
    \item \textbf{4 Hop}
        \begin{enumerate}
            \item \textbf{Q-7}: This query starts with a `Comment', and looks for
                this comment's creator by following the `CommentCreator' label.
                Then, we look at the tag superclasses of this person's interest
                tag by following `PersonInterestTag', `TagClass', and
                `TagClassSuperclass' labels.
        \end{enumerate}
\end{enumerate}

\medskip
As mentioned in the previous paragraph, we run these queries on the LDBC-SNB dataset
whose schema is provided in Appendix \ref{sec:schema}. However, LDBC provides
multiple datasets with the given schema, all of which are of varying sizes.
For this thesis, we use datasets with a scaling factor of 1 and 10 (SF-1 and
SF-10). As mentioned in Section \ref{sec:componentOverview}, we convert these
datasets to a binary format before processing. During this processing, we remove
edge and node-related information since that is not needed during the traversal.
After removing the unnecessary information, we partition the adjacency into
multiple files, each of which is then uploaded to S3.
Table \ref{table:dataSpecs} contains the specifications of the two datasets that
we use.


\begin{table}[h!]
 \centering
 \begin{tabular}{|c | c | c |} 
 \hline
  & SF-1 & SF-10 \\ [0.5ex] 
 \hline\hline
     \textbf{Nodes} & 3 Million & 30 Million \\ 
     \textbf{Bidirectional edges} & 17 Million & 170 Million \\
     \textbf{Number of partitions} & 37 & 173 \\
     \textbf{Total size} & 286 MB & 2.8 GB \\
 \hline
 \end{tabular}
 \caption{Dataset specifications}
 \label{table:dataSpecs}
\end{table}

\medskip
In order to run these queries, we run the graph access service and graph
algorithm service on an EC2 instances\cite{awsEC2}. AWS provides a variety of
compute instances for different workloads. In our case, we need a general
purpose compute instance which has high bandwidth. At the time of writing this
thesis, instances labeled `c7gn' fit these specifications the best. The
particular instance that we use for most of our benchmarks is labeled
`c7gn.xlarge' which provies a network bandwidth of 50 Gbps, has 8 Gigabytes of
RAM, has 4 vCPUs, and costs 0.25 \$/hour.

\section{Comparison with baseline}\label{sec:cmpBaseline}
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/charts/BFS_cmp_1}
        \caption{BFS SF-1}
        \label{fig:bfsSf1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/charts/BFS_cmp_10}
        \caption{BFS SF-10}
        \label{fig:bfsSf10}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/charts/DFS_cmp_1}
        \caption{DFS SF-1}
        \label{fig:dfsSf1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/charts/DFS_cmp_10}
        \caption{DFS SF-10}
        \label{fig:dfsSf10}
    \end{subfigure}
    \caption{Baseline comparison}
    \label{fig:baselineCmp}
\end{figure}
Figure-\ref{fig:baselineCmp} contains the comparison of our final
implemnetation, as described in Section \ref{sec:graphAccess} and Section 
\ref{sec:parallelAlgorithms}, with the baseline implementation described in
Section \ref{sec:baseline}. Figures \ref{fig:bfsSf1} and \ref{fig:bfsSf10}
compare the performance of Breadth first search for scaling factor of 1 and 10.
Similarly, Figures \ref{fig:dfsSf1} and \ref{fig:dfsSf10} do the same for Depth
first search. Note that the y-axis on these graphs is logarithmic. Looking at
these graphs, we notice the following facts:
\begin{enumerate}
    \item \textbf{No benefit for small graphs}: From figures \ref{fig:dfsSf1}
        and \ref{fig:bfsSf1}, it is clear that our proposed architecture perform
        worse than the simpler baseline implementation. This is primarily
        because the number of different files is quite small for SF-1 and
        therefore most of the files required for a traversal are loaded in the first
        few runs of that traversal. After that, all the traversals are performed
        from memory.
    \item \textbf{The proposed approach scales better}: If we compare the results of
        running DFS and BFS on SF-1 and SF-10, we notice that the running times
        for our final implementation remain relatively unchanged. However, the
        running times for the baseline implementation degrade by one to two
        orders of magnitude. Furthermore, the total time for running the entire
        workload for the baseline implementation on SF-10 with BFS was around 14
        minutes whereas the running time for our proposed implementation was
        just 24 seconds. This is a 35x improvement in the running time of the
        entire workload.
    \item \textbf{BFS $>$ DFS}: These results also show that the simpler parallel
        BFS implementation works much better than the parallel DFS
        implementation. For SF-10, the entire workload took 68 seconds while
        running DFS but took only 14 seconds while running BFS. We believe this
        is primarily due to the fact that the DFS implementation is more
        complicated, and uses more memory than the implementation for BFS.
\end{enumerate}

\medskip
Figure-\ref{fig:reqDist} shows the percentage of requests served by
the LRFU cache, the prefetcher, and AWS S3. We can see that the distribution is
very similar for both BFS and DFS. We also notice that the LRFU cache is very
effective and mitigates almost half of all the network calls to AWS S3. However,
we also notice that the prefetcher only serves around 3\% of all the
requests. We will now investigate why the prefetcher is less effective than 
we had hoped.
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/charts/pie_bfs}
        \caption{BFS}
        \label{fig:bfsPie}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/charts/pie_dfs}
        \caption{DFS}
        \label{fig:dfsPie}
    \end{subfigure}
    \caption{Request Distribution}
    \label{fig:reqDist}
\end{figure}

\subsubsection{Is the prefetcher useless?}\label{sec:prefetcherUseless}
To answer the question regarding the effectiveness of the prefetcher,
we need to remind ourselves that the traversal algorithms being run on the graph
algorithm service are parallel. This means that multiple units of concurrency
are trying to advance the traversal at the same time. This interferes with the
functioning of the prefetcher, which itself is trying to advance the traversal by
utilizing concurrency. This means these two mechanisms are competing against
each other. Since our implementation has more concurrency units in the 
parallel traversal algorithm than the prefetcher, the prefetcher is not very
beneficial.

\medskip
This reasoning can be verified by running sequential versions of
BFS and DFS. Figure \ref{fig:reqDistSeq} shows the request distribution when we
run sequential traversals instead of their parallel counterparts. We can see
that the prefetcher's share is substantially more. Interestingly, it turns out
that the prefetcher is more effective for DFS than for BFS. Since the sequential
implementations for BFS and DFS are equally efficient, we found that the
workload running time for DFS was 13 \% lower than the workload running time 
for BFS. However, the total running time for the sequential versions was still
significantly slower than their parallel counterparts. Parallel BFS was 82 \%
faster than sequential BFS, and Parallel DFS was 44 \% faster than sequential
DFS.
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/charts/pie_bfs_seq}
        \caption{BFS}
        \label{fig:bfsPieSeq}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/charts/pie_dfs_seq}
        \caption{DFS}
        \label{fig:dfsPieSeq}
    \end{subfigure}
    \caption{Request Distribution Sequential}
    \label{fig:reqDistSeq}
\end{figure}

\medskip
Given these results, we need to address whether it makes sense to have parallel
algorithms along with the prefetcher. If we want performance without regard for
hardware cost, it does indeed make sense to have both. However, in most cases,
there is a tradeoff to be made here. If we want faster traversals at the expense
of lower throughput, it makes sense to use only parallel algorithms. This is
because the traversal algorithm has more precise information, compared to the 
prefetcher, about the edges that need to be traversed. However, if we want
higher throughput by performing multiple traversals in different
concurrency units, it makes sense to use a prefetcher. This is because not
having the prefetcher
would free up more concurrency units for the graph algorithm service, thereby
increasing the throughput. For the rest of this thesis, however, we will still
use both the prefetcher and the parallel algorithms as they yield the best
performance when used together.

\subsubsection{Exploiting parallelism}
As we explained in Section \ref{sec:graphAccess}, one of our motivations for
introducing the modified CSR format was to enable more granular access to the
graph. With this granular access, we can make more requests for a given
bandwidth, this enables us to increase the number of traversals being
executed in parallel. In this section, we present the impact of running multiple
traversals in parallel.

\medskip
Figure-\ref{fig:parallelismImpact} shows the total workload running time for
different levels of parallelism. As the figure shows, we see significant
improvements in the running time. However, these improvements taper off at
higher levels of parallelism. By going from parallelism of 1 to 4, we see an
almost 4x improvement, however, going from parallelism of 10 to 20, we hardly
see any improvements. This suggests that although increasing the parallelism
helps, eventually the hardware becomes the limiting factor for the overall
running time. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/charts/parallelism}
    \caption{Parallelism impact}
    \label{fig:parallelismImpact}
\end{figure}

\medskip
At this stage, our implementation can process a workload consisting of
$7*20 = 140$ queries in 3 seconds whereas the baseline implementation processed
the same queries in 14 minutes. Even if we increase the parallelism for baseline
implementation to 2, the running time is still more than 7 minutes. Increasing
parallelism beyond this point is not feasible in the baseline implementation
because of its underlying architecture.

\subsubsection{Exploring AWS S3 One-Zone}
AWS S3 offers different storage classes for objects, and each storage class has
different performance characteristics. S3 One-Zone is one such storage class
that was introduced by AWS in the past few months, and according to AWS, it has
up to ten times lower latency than general-purpose storage. As our main goal
for this thesis was to find a way to reduce object access latency, we also
performed some of our experiments with S3 One-Zone.
Note that all the experiments
in the previous sections used S3's general-purpose storage, which
is S3's default storage type.

\medskip
Before we present our experimental evaluation, it is worth discussing how this
new storage type achieves ten times lower latency and the tradeoffs
associated with its use. AWS S3 is a region-specific service,
which means that all its servers along with the underlying data are present
in a single AWS region. For redundancy, each AWS region consists of multiple
availability zones, and each availability zone consists of multiple data
centers. Files stored as general-purpose storage are replicated across 
multiple availability zones, whereas files stored as One-Zone storage type are
replicated only within a single availability zone. This architectural
distinction gives rise to the following changes:  
\begin{enumerate}
    \item \textbf{Lower latency}: Since the physical distance between the
        servers is reduced, the network latency is also lower. However, this
        also implies that the AWS EC2 instances processing the data must be
        in the same availability zone.
    \item \textbf{Lower resilience to outages}: Since the general purpose
        storage is replicated across multiple availability zones, an outage in
        one of the availability zones is unlikely to have an operational impact
        on the service. On the other hand, if an object is stored in a single-zone, 
        then it is not resilient to single-zone outages. However, it must
        be noted that the last time AWS S3 had an outage was in 2017.
    \item \textbf{Higher storage cost and lower request cost}: At the time of
        writing this thesis, the cost of
        storage for S3 One-Zone is 0.16 \$/GB/month, whereas the cost for S3
        General is just 0.023 \$/GB/month. However, the cost of a GET request for
        S3 One-Zone is lower than that for S3 General. A thousand requests to
        S3-One-Zone cost 0.0002 \$, whereas a thousand requests to S3-General
        cost 0.0004 \$. 
\end{enumerate}

\medskip
With these tradeoffs in mind, we present the performance of S3-One-Zone for our
workload. Figure-\ref{fig:ozCmp} shows the average running times of traversals
on the SF-10 dataset with inter-traversal parallelism of 1. We observe that the
running time is reduced by almost an order of magnitude. The total workload
running time was 83 \% lower for BFS and 82 \% lower for DFS. Therefore, we see
consistent improvements across the workload. However, these improvements become less
substantial as we increase the level of inter-traversal parallelism because the
bottleneck shifts to the data processing capabilities of the underlying
instance.
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/charts/BFS_cmp_oz}
        \caption{BFS}
        \label{fig:bfsCmpOz}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/charts/DFS_cmp_oz}
        \caption{DFS}
        \label{fig:dfsCmpOz}
    \end{subfigure}
    \caption{Comparing S3 General with S3 One-Zone}
    \label{fig:ozCmp}
\end{figure}

\subsection{Optimizing partition sizes}\label{sec:partitionSize}
\subsection{Distributed Deployment}\label{sec:distDeploy}
\section{Comparison with other tools}\label{sec:cmpOtherTools}
\subsection{Neo4j}
\subsubsection{Monolithic Deployment}
\subsubsection{Distributed Deployment}
\subsection{Apache Flink}

