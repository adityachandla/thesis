The size of graphs used in academia and industry is increasing rapidly, and as a result, 
it has become common for graphs to have tens of
billions of edges\cite{sahu2017ubiquity}. This trend is expected to
continue as the amount of data being collected and processed increases. A recent
survey shows that scalability is already the primary concern of graph database
users\cite{sahu2017ubiquity}. As the scale of graphs
increases further, decoupling storage from graph query evaluations using distributed
cloud storage services like AWS S3\cite{awsS3} may be
more cost-efficient. These services provide durable data storage, high
throughput, and a pay-as-you-go model where you only pay for the data you store
and the number of requests you make. Using these characteristics would
enable graph databases to provide more flexible scaling. Furthermore, since
these services ensure availability and durability, they would alleviates 
the costs of maintaining redundant infrastructure for reliability. 
Therefore, it is worthwhile to explore the feasibility of such database architectures.

\medskip
The idea of having separate storage and compute has been studied in the context
of relational databases. In 2008, Brantner et al.\cite{brantner2008building}
described how a relational database could be built on AWS S3 while 
providing atomicity, isolation, and durability.
Snowflake\cite{dageville2016snowflake}, in 2016, was
one of the first databases that fully realized the idea of having
separate storage and compute layers. This idea was adopted by other databases
like AWS Aurora\cite{verbitski2017amazon} in 2017 and more recently by
Neon\cite{neonPostgres} in 2021. Despite the existence of so many relational databases
that separate storage and compute layers, this idea remains unexplored for
native graph databases.

\medskip
Although many native graph databases enable scaling out
using sharding\cite{besta2023demystifying}, the compute and storage are
still tied together. Unlike such architectures, this paper explores an
architecture where the storage of raw graphs is delegated to a distributed cloud
store and, memory of compute instances is only used to process queries and to store
intermediate results.

\medskip
There are two main challenges when considering databases with separate compute
and storage: Distribution transaction management and access latency. The first
challenge, although more intricate, has been studied in the context of
relational databases\cite{brantner2008building} and can be extended for other
systems that follow this general architecture. Therefore, in this paper, we
explore how to reduce the access latency for graph-specific operations in
the context of graphs stored on distributed file systems.

\medskip
To study the impact of our techniques on access latency, we use
Breadth-first search (BFS) and Depth-first search (DFS) to
benchmark the performance. These two traversal algorithms are the most
commonly used algorithms for performing graph traversals\cite{sahu2017ubiquity}.
We will run these algorithms with a bounded depth and thus, only
access a small part of the graph. On the spectrum of graph algorithms from
Online transaction processing (OLTP)
to Online analytical processing (OLAP) as described by 
Besta et al.\cite{besta2023demystifying}, these 
traversal algorithms lie
more towards the OLTP side as the portion of the graph explored by a single traversal
would be much smaller than the size of the graph.

\medskip
To minimize the latency, we propose a graph storage format that
would provide granular access over the graph stored in AWS S3. Then, we
extend some of the existing caching techniques to lower the latency of
graph access. Finally, we will describe the cases where it is viable and more
cost-efficient to use an architecture that separates storage from compute.

\section{Research Objectives}
In this paper, we provide an initial analysis of how distributed storage services can 
be used to perform traversals on large graphs. While using these distributed storage services, the primary
issue to address is latency. The latency of accessing data in a networked
distributed system involves communication over the network, which is at least three orders of
magnitude more than accessing data from local storage. In return for this latency,
we get virtually unlimited throughput as read operations are distributed across a cluster 
of thousands of physical machines or more. Therefore, in this paper, we will evaluate ways to
reduce the latency of accessing graphs and quantify the performance
benefits of increased throughput capacity by employing parallelism. We will focus on the 
performance of two commonly used graph traversal algorithms: Breadth-first search 
(BFS) and Depth-first search (DFS). More formally, the first research
objective is as follows:
\begin{displayquote}
    \textbf{RO 1:} Gauge the effectiveness of caching and prefetching techniques
    on the latency of graph access for graph traversals and the effectiveness of
    using parallelism to increase the throughput of these traversals.
\end{displayquote}

\medskip
Apart from reducing the latency, we will also discuss how the cost model of
distributed storage engines differs from the traditional model of coupled
compute and storage. For storage engines like S3, we are charged for every
gigabyte of storage and for every request that we make on that
storage. On the other hand, in the case of an SSD/HDD storage unit, you pay for a
certain amount of storage, and there is no additional cost for accessing the
storage. Therefore, we aim to provide a model to help users choose between one
form of storage or the other. More formally, the second research objective is
as follows:
\begin{displayquote}
    \textbf{RO 2:} Provide a cost model to help users decide whether using S3
    instead of SSD/HDDs might be more cost-effective.
\end{displayquote}

\section{Research contributions}
In line with the research objectives mentioned in the previous section, the 
unique contributions of this research are as follows:
\begin{enumerate}
    \item We provide the details of an architecture that can be used to perform 
        traversals on large graphs using a distributed cloud storage service. 
        (Section \ref{sec:componentOverview}). 
    \item We explore the following performance optimizations to lower the latency and
        increase the throughput of these traversals:
        \begin{enumerate}
            \item Using a modified CSR format. (Section \ref{sec:modifiedCsr})
            \item Utilizing LRFU cache and a graph aware prefetcher 
                (Section \ref{sec:accessCachePrefetching}).
            \item Parallelizing traversals (Section
                \ref{sec:parallelAlgorithms})
        \end{enumerate}
    \item We compare the performance of the proposed architecture with:
        \begin{enumerate}
            \item A baseline solution without any of the proposed optimizations.
                (Section \ref{sec:cmpBaseline})
            \item A popular graph database, Neo4j. (Section \ref{sec:neo4jCmp})
            \item A popular big data processing framework, Apache Flink.
                (Section \ref{sec:flinkCmp})
        \end{enumerate}
    \item We evaluate the impact of partition sizes and object types on the performance of
        traversals. (Section \ref{sec:partitionSize})
    \item We measure the performance improvements with distributed deployments
        of the proposed architecture. (Section \ref{sec:distDeploy})
    \item We provide a cost model to enable users and developers of graph
        databases to check when using a service like AWS S3 would be more
        cost-efficient. (Section \ref{sec:costModel})
    \item We identify use cases where using our proposed architecture is
        cheaper than traditional architectures. (Section \ref{sec:useCases})
\end{enumerate}


\section{Structure of the thesis}

\medskip
Chapter \ref{chapter:preliminaries} provides the necessary background for
this thesis. Section \ref{sec:distributedStorage} talks about the
history, architecture, and characteristics of distributed storage systems. We
also give a brief overview of the capabilities of AWS S3, which is the service
we use in this thesis. Then
Section \ref{sec:serverlessArch} discusses the advantages of database
architectures that separate compute and storage. Finally, 
Section \ref{sec:cachingDistSys} discusses the caching and prefetching
strategies that were employed in this thesis to lower the latency of graph
traversals.

\medskip
Chapter \ref{chapter:systemArchitecture}, introduces our architecture for
performing traversals. We start by providing a description of each component and
its responsibilities in Section \ref{sec:componentOverview}. 
Then, Section \ref{sec:baseline} provides a baseline implementation, 
which will be useful to gauge
the impact of the improvements made in the subsequent sections. Finally, 
Sections \ref{sec:graphAccess} and \ref{sec:parallelAlgorithms} 
describe the details of our proposed architecture
that enables low-latency and high-throughput traversals for large graphs.

\medskip
In Chapter \ref{chapter:evaluation}, we present benchmarks showcasing the
effectiveness of our implementation and the effectiveness of the underlying 
system architecture. We begin this chapter by
comparing the performance with the baseline solution in
Section \ref{sec:cmpBaseline}. Then in Section \ref{sec:cmpOtherTools}, we
compare the performance of our solution with Neo4j and Apache Flink. This
section highlights various characteristics of different types of tools (GDBMS,
RDBMS, Big Data tools, and Custom Solutions) and the areas in which they 
are suitable. Finally, in Section \ref{sec:keyTakeaways}, we summarize the key
takeaways from this chapter.

\medskip
Chapter \ref{chapter:discussion} provides concrete use cases for the use of our
proposed architecture and considers some threats to the credibility of this
thesis. Section \ref{sec:costModel} provides a formal cost model for reasoning
about the cost difference between using AWS S3 versus coupled storage. In this
section, we also provide use cases where using one architecture over the other
may be more cost-efficient. In Section \ref{sec:threats}, we discuss the threats
to the credibility of this research. This section talks about how some of the
aspects of a graph database that we have not considered in this thesis may
impact the viability of AWS S3 as a graph storage layer. 

\medskip
Chapter \ref{chapter:conclusions} concludes the thesis and
suggests possible directions for future work. This section contains information
about how we may be able to extend this work to reach a point where we have a
fully functioning graph database whose storage resides in S3.
