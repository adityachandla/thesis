In a recent survey titled `Ubiquity of large graphs'\cite{sahu2017ubiquity}, the
authors noted that the size of graphs being used in both academia and industry
is increasing. Although this survey found that it was common for graphs to have
tens of billions of edges, it also found that scalability was the primary
concern of all the participants surveyed\cite{sahu2017ubiquity}. In this thesis,
we evaluate one way to alleviate this concern of scalability by decoupling
storage of base graph from the query evaluation.

\medskip
Today, all major cloud providers have a distributed cloud storage offering,
Google cloud platform offers Google cloud storage\cite{gcpStorage}, Azure offers
Azure Blob storage\cite{azureStorage}, and AWS offers Simple Storage Service or 
S3\cite{awsS3}. These distributed storage services are already used by Big Data
Analytics tools like Snowflake\cite{snowflake} and RDBMSs like Neon\cite{neonPostgres} 
for storage of underlying data. The use of these distributed storage services has not
yet been explored for storing and querying graph data.

\medskip
In this paper, we provide an initial analysis of how distributed storage services can 
be used to query large graphs. While using these distributed storage services, the primary
issue that needs to be addressed is of latency. The latency of accessing data in a networked
distributed system involves communication over the network which is at least three orders of
magnitude more than accessing data from local storage. In return for this increased latency,
we get almost unlimited throughput as read operations are distributed across a cluster 
of thousands of physical machines if not more. Therefore, in this paper, we evaluate ways to
reduce the latency of accessing graphs.More precisely, we will focus on the 
performance of two commonly used graph traversal algorithms: Breadth first search 
(BFS) and Depth first search (DFS). We use these algorithms on LDBC datasets
to provide results for multi-hop traversals. 

\section{Structure of the thesis}

\medskip
In Chapter-\ref{chapter:preliminaries}, we provide the necessary background for
this thesis. In Section-\ref{sec:distributedStorage} we talk about the
history, architecture, and characteristics of distributed storage systems. We
also give a brief overview of the capabilities of AWS S3, which is the service
that we use in this thesis. Then,
in Section-\ref{sec:serverlessArch}, we discuss the advantages of database
architecutres which separate compute and storage. Finally, in
Section-\ref{sec:cachingDistSys} we discuss the caching and prefetching
strategies that we employ in this thesis to lower the latency of graph
traversals.

\medskip
In Chapter-\ref{chapter:systemArchitecture}, we introduce our architecture for
performing traversals. We start by providing a description of each component and their
responsibilities in Section-\ref{sec:componentOverview}. After that, we describe
the traversal queries which will be evaluated by our system in Section-\ref{sec:queryEvaluation}.
Then we provide a baseline implementation which we will be useful for comparing
the impact of the improvements made in the subsequent sections. Finally, in
Sections-\ref{sec:graphAccess}, \ref{sec:parallelAlgorithms}, and
\ref{sec:partitionSize}, we describe the details of our proposed architecture
which enables low latency traversals for large graphs.

\medskip
In Chapter-\ref{chapter:evaluation}, we present the results of the
implementation of the proposed system architecture. We begin this chapter by
comparing the performance with the baseline solution in
Section-\ref{sec:cmpBaseline}. Then in Section-\ref{sec:cmpOtherTools}, we
compare the performance of our solution with Neo4j and Apache Flink. This
section highlights various characteristics of different types of tools(GDBMS,
RDBMS, Big Data tools, and Custom Solutions) and the areas in which they 
are suitable. 

\medskip
In Chapter-\ref{chapter:discussion}, we elucidate the reasoning for various choices
made throughout the thesis.
In Section-\ref{sec:altArchitectures} we discuss other possible architectures for
performing traversals on large graphs using S3 and their advantages and
disadvantages over the proposed architecture. Then, in
Section-\ref{sec:useCases}, we discuss the use cases where this architecture
can be more cost efficient and flexible compared to other tools and where users
would be better off avoiding this architecture. Finally, in
Section-\ref{sec:threats}, we consider the threats to the credibility of this
work.

\medskip
Finally, in Chapter-\ref{chapter:conclusions}, we conclude the thesis and
suggest possible directions for future work. This section contains information
about how we may be able to extend this work to reach a point where we have a
fully functioning graph database whose storage resides in S3.
