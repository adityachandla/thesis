\section{Component Overview}\label{sec:componentOverview}
Figure-\ref{fig:sysArch} contains the high level overview of the system that
we will use for query evaluation. We will now give a brief explaination of the
components present in this figure:
\begin{enumerate}
    \item \textbf{LDBC}: The Linked Data Benchmark Council(LDBC) publishes datasets
        for various types of graph processing workloads. In this thesis, we make
        use of the Social Network Benchmark (SNB) datasets in order to evaluate
        the performance of our traversals.
    \item \textbf{LDBC Converter}: The LDBC dataset are in csv format and
        contain additional information about nodes and edges. As we will explain
        in Section-\ref{sec:modifiedCsr}, we need the data to be in a particular
        binary format and we do not have any use for node and edge properties.
        This component of the system removes the unnecessary information from
        the LDBC datasets and maps the data into the desired binary format.
    \item \textbf{AWS S3}: Once the LDBC converter converts the data into a
        binary format, this data is added to an S3 bucket. A `bucket' in AWS S3
        is a container for files.
    \item \textbf{Graph Access Service}: This service provides an interface for
        accessing a graph stored in AWS S3. This interface provides the users of
        this interface with the ability to get a node's neighbours by providing
        the source node, edge label, and edge direction. This service contains
        all the caching mechanisms that we will use to reduce the latency of
        traversals. We will discuss this service in more detail in
        Section-\ref{sec:graphAccess}.
    \item \textbf{Graph Algorithm Service}: This service is responsible for
        using the interface provided by the graph access service to perform
        traversals and measure their performance. We discuss this component in
        more detail in Section-\ref{sec:parallelAlgorithms}.
\end{enumerate}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/architecture.png}
    \caption{High Level System Architecture}
    \label{fig:sysArch}
\end{figure}

Looking at the architecture in Figure-\ref{fig:sysArch}, it is natural to wonder
why do we have two separate services for accessing the graph and performing
traversals on that graph. There are two reasons for this separation: First, it
separates the logic for accessing graphs from the logic for performing
traversals with minimal overhead; Second, this type of architecture is
particularly amenable to creation of a multi-tenant storage layer as used
in the Neon database\cite{neonPostgres}. This is why we have chosen to separate
the service responsible for accessing the graph and caching from the service
responsible for running traversal algorithms.

\section{Baseline Implementation}\label{sec:baseline}
Since there are no existing tools that we can use to compare our solution with,
we present a baseline implementation to gauge the effectiveness of our caching
techniques and algorithmic improvements that we will present in the next
sections. The main idea of the baseline implementation is to provide the most
straightforward way to perform traversals on a graph that is stored in AWS S3.

\medskip
As per the workflow we presented in Section-\ref{sec:componentOverview}, we
first need to convert the graph from the LDBC dataset into a format which
enables us to fetch parts of the graph that we need for processing. In order to
do this, we convert the graph into an adjacency list, then we partition that
adjacency list based on size, and then finally convert it to compressed sparse
row (CSR) format\cite{duff1984computer}. With this partitioned CSR format stored
in S3, our aim is to load the desired file whenever we want to fetch a node's
neighbours.

\medskip
Figure-\ref{fig:baselineImpl} shows the design of graph access service and graph
algorithm service for this baseline implementation. The graph algorithm service
is responsible for performing the traversals and recording the results. This
service uses the textbook implementation of DFS and BFS, with the only caveat
that a node's neighbours are requested from the graph access service. The graph
access service contains an index which maps node ranges to S3's file names. With
this index, we can get the file name for the S3 file which contains a particular
node's neighbours. Additionally, this service also contains an LRU cache of some
files that were fetched from S3. The size of this cache is limited because each
cache entry contains an entire file with adjacency information related to
hundreds of nodes. With these components in place, whenever the graph access
service gets a request, it first checks if that request can be served from the
cache, if not, we fetch the file corresponding to the requested node and add
that file to the cache.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/baseline.png}
    \caption{Baseline Implementation}
    \label{fig:baselineImpl}
\end{figure}

\medskip
The approach mentioned in this section is quite straightforward but there are
various improvements that can be done to improve the performance and to better 
leverage the features provided by AWS S3. In the next few sections, we will
discuss how we can improve the implementation for both the graph access service
as well as the graph algorithm service. 

\section{Graph access service}\label{sec:graphAccess}
In this section, we first describe a modification of the CSR format in
Section-\ref{sec:modifiedCsr} which would help us improve our cache performance.
Then, in Section-\ref{sec:accessCachePrefetching}, we describe how we leverage
this updated file format and employ the caching schemes mentioned in
Section-\ref{sec:cachingDistSys}.

\subsection{Modified CSR structure}\label{sec:modifiedCsr}
In the baseline implementation of graph access service described in
Section-\ref{sec:baseline}, we had to download entire files in order to access
a node's neighbours. This approach suffers from a couple of problems which makes
fetching inefficient and restrictive. The first problem is that we have to
fetch and parse the information for an entire file in order to access a single
node's neighbours which is wasteful. Furthermore, this approach hinders our
ability to have a  higher level of granularity for better cache performance.
Secondly, this approach makes it harder to utilize the fact that S3 replicates
files to various servers. Due to these problems, we decided to modify the
underlying binary format and the way we fetch a node's neighbours.

\medskip
Figure-\ref{fig:csrFormat} shows the binary format that we use for fetching a
node's neighbour. This format can logically be divided into three layers. The
first layer, which is always of a constant size, contains the first and last
node identifier present in this file. The second layer, contains the byte offset
for the first incoming edge for a node and the first outgoing edge for a node.
Since these offsets are of a constant size and the fact that the first layer of
the header tells us how many nodes are present in this file, we can calculate
the size of this second layer. The third and final layer contains the edge
information, which in our case consists of the edge label and the edge
desitnation. This type of structure closely resembles a traditional CSR format
except for the fact that we store both incoming and outgoing edges in the same
array and the fact that we store byte offsets instead of array indices.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/csrFormat.png}
    \caption{Updated CSR Format}
    \label{fig:csrFormat}
\end{figure}

\medskip
The aforementioned format gives us the ability to fetch a node's neighbours
without having to download an entire file. With this modified structure, we can
store the first layer for every file in memory and fetch the second layer of a
file when it is first accessed. After doing so, we would be able to fetch a
node's incoming or outgoing neighbours without any additional overhead.
Furthermore, we can now parallelize requests to a single file which would lead
to better throughput since these requests would likely be distributed to
different instances within S3. As an additional benefit, we can now perform
caching on a more granular level since we do not incur the overhead of fetching
and parsing an entire file whenever we need to access a node's neighbours.

\subsection{Caching and Prefetching}\label{sec:accessCachePrefetching}
Figure-\ref{fig:graphAccessArch} shows the final architecture of the graph
access service. The service consists of three basic parts:
\begin{enumerate}
    \item \textbf{Interfaces for external communication}: There are two 
        components which provide an interface with external resources and are
        colored grey in the figure. The first component, on the left,
        exposes a gRPC interface which receives requests for accessing a node's
        neighbours, starting traversals, and ending traversals. The second
        component, on the right, is responsible for accessing S3. This component
        takes a filename along with starting and ending file offsets and returns
        the requested file content.
    \item \textbf{Orchestration}: There are two components that help with
        orchestration of request and are colored blue in the figure. The first
        component is the `Request Processor' which is responsible for
        interacting with the caches, moving data between caches, and fetching
        data from S3 in case it is not present in the caches. The second
        component titled `Vertex offsets' contains metadata related to the CSR
        format that we described in Section-\ref{sec:modifiedCsr}.
    \item \textbf{Caches}: There are two different types of caches that are used
        in this service and are colored green in the figure. The first of these
        components is an LRFU cache which is used to store nodes that were
        previously accessed. The second component is a per-query prefetcher
        which fetches the nodes that are likely to be accessed by a client in
        the near future. The rest of this section is devoted to the description
        of these two components.
\end{enumerate}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/graphAccessServiceFinal.png}
    \caption{Graph Access Service}
    \label{fig:graphAccessArch}
\end{figure}

% TODO describe these things.
\medskip
The \textbf{LRFU cache} is a relatively simple 

\section{Prallelizing graph algorithm service}\label{sec:parallelAlgorithms}
One of the main advantages of using AWS S3 is that it is able to sustain a very
high throughput. The caching mechanisms mentioned in the previous sections
reduce the latency for accessing a node's neighbours and mitigate expensive
network calls to S3 in the hot-path. These components do help with reducing
latency but they do not do anything to ensure that make use of the
high-throughput provided by AWS S3. In order to do that, we parallelize our
traversal algorithms. In this section, we will present parallel implementations
of BFS and DFS that would help us speed up our traversals by utilizing the
high-throughput of AWS S3.

\subsection{Parallel BFS implementation}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/parallelBFS.png}
    \caption{Parallel BFS}
    \label{fig:parallelBFS}
\end{figure}
\subsection{Parallel DFS implementation}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/parallelDFS.png}
    \caption{Parallel DFS}
    \label{fig:parallelDFS}
\end{figure}
